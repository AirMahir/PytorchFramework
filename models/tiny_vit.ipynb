{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1e979af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "# All classes are refactored to match timm's naming conventions.\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits image into patches and embeds them.\n",
    "    This class now matches the 'patch_embed' module in timm's ViT.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embedding_dim=192):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        # Changed 'patching' to 'proj' to match timm\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embedding_dim,\n",
    "            kernel_size=patch_size, # kernel_size should be patch_size\n",
    "            stride=patch_size,\n",
    "            padding=0\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, C, H, W) -> (B, embedding_dim, H', W')\n",
    "        x = self.flatten(x) # (B, embedding_dim, H', W') -> (B, embedding_dim, num_patches)\n",
    "        x = x.permute(0, 2, 1) # (B, embedding_dim, num_patches) -> (B, num_patches, embedding_dim)\n",
    "        return x\n",
    "\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention module -- MODIFIED to match timm's architecture.\n",
    "    This version uses a single Linear layer for Q, K, V for efficiency and\n",
    "    to allow direct weight loading from timm.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=192, num_heads=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # MODIFICATION: A single Linear layer for Q, K, and V\n",
    "        # The output dimension is 3 * embedding_dim because it holds Q, K, and V concatenated.\n",
    "        self.qkv = nn.Linear(embedding_dim, embedding_dim * 3, bias=True)\n",
    "\n",
    "        # The projection layer remains the same\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, n_patches, dim = x.shape\n",
    "\n",
    "        # MODIFICATION: Project x once to get q, k, and v together\n",
    "        # (B, N, D) -> (B, N, 3*D)\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        # Reshape and split qkv into q, k, and v for multi-head attention\n",
    "        # (B, N, 3*D) -> (B, N, 3, num_heads, head_dim)\n",
    "        qkv = qkv.reshape(batch_size, n_patches, 3, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Permute to (3, B, num_heads, N, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        \n",
    "        # Split into q, k, v. Each will have shape (B, num_heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        # (B, num_heads, N, head_dim) @ (B, num_heads, head_dim, N) -> (B, num_heads, N, N)\n",
    "        attn_score = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn_score.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # (B, num_heads, N, N) @ (B, num_heads, N, head_dim) -> (B, num_heads, N, head_dim)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(batch_size, n_patches, dim)\n",
    "        \n",
    "        # Final projection\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP block. Refactored to use named 'fc1' and 'fc2' like timm.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=192, mlp_ratio=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        mlp_hidden_dim = int(embedding_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embedding_dim, mlp_hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(mlp_hidden_dim, embedding_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Encoder block.\n",
    "    This structure now directly mirrors timm's 'blocks.i' module.\n",
    "    It contains norm1, attn, norm2, and mlp.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=192, num_heads=3, mlp_ratio=4., dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.attn = MultiheadSelfAttention(\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.mlp = MLP(\n",
    "            embedding_dim=embedding_dim,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-Norm architecture\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VitTiny(nn.Module):\n",
    "    \"\"\"\n",
    "    The main Vision Transformer class.\n",
    "    All module names are now aligned with the timm library.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 img_size=224,\n",
    "                 in_channels=3,\n",
    "                 patch_size=16,\n",
    "                 num_transformer_layers=12,\n",
    "                 embedding_dim=192,\n",
    "                 mlp_ratio=4,\n",
    "                 num_heads=3,\n",
    "                 num_classes=1000, # Default for ImageNet-1k\n",
    "                 dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Renamed 'patch_embed_layer' to 'patch_embed'\n",
    "        self.patch_embed = PatchEmbeddings(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        \n",
    "        # Renamed 'cls' to 'cls_token'\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "        \n",
    "        # Renamed 'pos_emb' to 'pos_embed'\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embedding_dim))\n",
    "        self.pos_drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Renamed 'enc_blocks' to 'blocks'\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerEncoderBlock(\n",
    "                embedding_dim=embedding_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_transformer_layers)])\n",
    "        \n",
    "        # Final LayerNorm and classifier head, matching timm's naming\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        self.head = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Prepend class token\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Classifier head only uses the class token\n",
    "        cls_token_final = x[:, 0]\n",
    "        x = self.head(cls_token_final)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43205a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "433727ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_config(config_file_path: str):\n",
    "    try:\n",
    "        with open(config_file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    except:\n",
    "        return FileNotFoundError(\"The config file is corrupted/absent\")\n",
    "    \n",
    "configs = read_config(r\"C:\\Users\\e87299\\Desktop\\Training\\Week2-Pytorch\\Framework-pytorch\\configs\\configs_classification.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d760919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\e87299\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading timm model state_dict...\n",
      "Loading state_dict into custom model...\n",
      "\n",
      "Weights loaded successfully using direct method!\n",
      "Test forward pass successful. Output shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "\n",
    "my_vit_perfect_replica = ViT(config=configs[\"model\"])\n",
    "\n",
    "# my_vit_perfect_replica = VitTiny(num_classes=1000)\n",
    "\n",
    "\n",
    "# 2. Create the timm model to get its state dictionary\n",
    "print(\"Loading timm model state_dict...\")\n",
    "timm_model = timm.create_model(\n",
    "    'vit_tiny_patch16_224.augreg_in21k_ft_in1k',\n",
    "    pretrained=True,\n",
    "    num_classes=2\n",
    ")\n",
    "timm_state_dict = timm_model.state_dict()\n",
    "\n",
    "# 3. Load the weights directly!\n",
    "# This will now work because every key and tensor shape matches perfectly.\n",
    "print(\"Loading state_dict into custom model...\")\n",
    "my_vit_perfect_replica.load_state_dict(timm_state_dict, strict=True)\n",
    "\n",
    "print(\"\\nWeights loaded successfully using direct method!\")\n",
    "\n",
    "# Verify the model works\n",
    "my_vit_perfect_replica.eval()\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    output = my_vit_perfect_replica(dummy_input)\n",
    "\n",
    "print(f\"Test forward pass successful. Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554c1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
